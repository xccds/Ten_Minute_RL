{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十分钟强化学习第十一讲：DDPG方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy-based + Value-based方法\n",
    "- 先使用经验池数据训练Q网络\n",
    "- 再基于Q网络训练策略网络\n",
    "- 在action中增加噪音进行探索\n",
    "- 适合于连续的行动空间场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-2.0, 2.0, (1,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01685185, -0.999858  ,  0.55068326], dtype=float32), {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03430643, -0.99941134, -0.3492102 ], dtype=float32),\n",
       " -2.5519544811313466,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([-1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pocily_net(nn.Module):\n",
    "    def __init__(self, input_size, h1_size, h2_size,output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, h1_size)\n",
    "        self.linear2 = nn.Linear(h1_size, h2_size)\n",
    "        self.linear3 = nn.Linear(h2_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value_net(nn.Module):\n",
    "    def __init__(self, input_size, h1_size,h2_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, h1_size)\n",
    "        self.linear2 = nn.Linear(h1_size+output_size, h2_size)\n",
    "        self.linear3 = nn.Linear(h2_size, 1)\n",
    "\n",
    "    def forward(self, x, action):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = torch.cat((x, action), dim = 1)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise:\n",
    "\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_space, action_space,h1_size = 200,h2_size = 100,  gamma = 0.99,\n",
    "                max_memory=50000, lr=0.001):\n",
    "        self.memory = deque(maxlen=max_memory) \n",
    "        self.gamma = gamma\n",
    "        self.online_value_model = Value_net(state_space,h1_size,h2_size,action_space)\n",
    "        self.target_value_model = Value_net(state_space,h1_size,h2_size,action_space)\n",
    "        self.online_policy_model = Pocily_net(state_space,h1_size,h2_size,action_space)\n",
    "        self.target_policy_model = Pocily_net(state_space,h1_size,h2_size,action_space)\n",
    "        self.value_optimizer = optim.Adam(self.online_value_model.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.online_policy_model.parameters(), lr=lr)\n",
    "        self.noise = Noise(action_space)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.copy_model()\n",
    "\n",
    "    def copy_model(self):\n",
    "        self.target_value_model.load_state_dict(self.online_value_model.state_dict())\n",
    "        self.target_policy_model.load_state_dict(self.online_policy_model.state_dict())\n",
    "\n",
    "    def train_step(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        Q_value = self.online_value_model(states,actions)\n",
    "\n",
    "        next_policy_action = self.target_policy_model(next_states)\n",
    "        next_Q_value = self.target_value_model(next_states, next_policy_action)\n",
    "\n",
    "        target_Q_value = (rewards + self.gamma * next_Q_value * (1 - dones))\n",
    "\n",
    "        value_loss = self.criterion(Q_value,target_Q_value)\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        policy_action = self.online_policy_model(states)\n",
    "        policy_action_q = self.online_value_model(states,policy_action)\n",
    "        policy_loss = -policy_action_q.mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) \n",
    "\n",
    "    def train_long_memory(self,batch_size):\n",
    "        if len(self.memory) > batch_size:\n",
    "            mini_sample = random.sample(self.memory, batch_size) # list of tuples\n",
    "\n",
    "            states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "            states = np.array(states)\n",
    "            actions = np.array(actions)\n",
    "            next_states = np.array(next_states)\n",
    "            experiences = self.load((states, actions, rewards, next_states, dones))\n",
    "            self.train_step(experiences)\n",
    "\n",
    "\n",
    "    def get_action(self, state, add_noise = True):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        action = self.online_policy_model(state).detach().numpy()\n",
    "        action *= 2\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "\n",
    "        return np.clip(action, -2, 2)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        states = torch.tensor(states, dtype=torch.float)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float)\n",
    "        actions = torch.tensor(actions, dtype=torch.float)\n",
    "        #actions = torch.unsqueeze(actions, -1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float)\n",
    "        rewards =torch.unsqueeze(rewards, -1)\n",
    "        dones = torch.tensor(dones, dtype=torch.long)\n",
    "        dones =torch.unsqueeze(dones, -1)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, max_game=1000,  max_step=200, evl_step = 100):\n",
    "    agent = Agent(state_space = 3, action_space = 1)\n",
    "    scores = []\n",
    "\n",
    "    for i in  range(max_game):\n",
    "\n",
    "        state_new, _ = env.reset()\n",
    "        agent.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        for t in range(max_step):\n",
    "            state_old = state_new\n",
    "            action = agent.get_action(state_old)\n",
    "            state_new, reward, done, _, _ = env.step(action)\n",
    "            agent.remember(state_old, action, reward, state_new, done)\n",
    "            agent.train_long_memory(batch_size=256)\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.copy_model()\n",
    "        scores.append(score)\n",
    "\n",
    "        if (i>0) and (i % evl_step ==0):         \n",
    "            print(\"Running episode  {}, avg reward {:.2f}. \".format(\n",
    "                i, np.mean(scores[-100:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode  100, avg reward -499.67. \n",
      "Running episode  200, avg reward -176.77. \n",
      "Running episode  300, avg reward -166.13. \n",
      "Running episode  400, avg reward -172.31. \n",
      "Running episode  500, avg reward -180.20. \n",
      "Running episode  600, avg reward -179.80. \n",
      "Running episode  700, avg reward -173.97. \n",
      "Running episode  800, avg reward -185.10. \n",
      "Running episode  900, avg reward -170.61. \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "train(env) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
